# -*- coding: utf-8 -*-
"""ML Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10F3XoUByc9q-VbLdJCINE7lKHSyXUWxZ
"""

import statsmodels.api as sm
import statsmodels.formula.api as smf
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns


#ADD DATASET BELOW
raw0 = pd.read_csv('YOUR DATASET')
raw0.dropna()
raw0.default=pd.get_dummies(raw0.default,drop_first=True) # default = 1
raw0.student=pd.get_dummies(raw0.student,drop_first=True) # student = 1

#MATCH YOUR DATA SET HERE
Y = raw0.default
X = raw0.iloc[:,2:]
X = sm.add_constant(X)

#TRAINING DATASET

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3) #spare 30% of data for test set
logitres = LogisticRegression(fit_intercept=0, solver = 'lbfgs').fit(X_train, y_train)

y_pred = logitres.predict(X_test)
cm_logit = confusion_matrix(y_test, y_pred)

#CORRECT CLASSIFICATION GRAPH

xaxis = ["Correct Classifications", "Incorrect Classifications"]
yaxis = cm_logit[0][0], cm_logit[1][1]
  
# making the bar chart on the data
plt.bar(xaxis, yaxis)
  
# calling the function to add value labels

plt.ylabel("Count")
  
# giving title to the plot
plt.title("Counts of Incorrect vs Correct Classifications")
  
# giving X and Y labels
plt.ylabel("Count")
  
# visualizing the plot
plt.show()

#TYPE 1 AND 2 ERRORS GRAPH

xaxis = ["Type 1 Errors", "Type 2 Errors"]
yaxis = cm_logit[0][1], cm_logit[1][0]
  
# making the bar chart on the data
plt.bar(xaxis, yaxis)
  
# calling the function to add value labels
plt.xlabel("Type Of Error")
plt.ylabel("Count")
  
# giving title to the plot
plt.title("Counts of Type 1 & 2 Errors")
  
# giving X and Y labels
plt.xlabel("Type of Error")
plt.ylabel("Count")
  
# visualizing the plot
plt.show()

#PRECISION AND RECALL GRAPH

xaxis = ["Precision", "Recall"]
yaxis = [cm_logit[1][1]/(cm_logit[1][1]+cm_logit[0][1]), cm_logit[1][1]/(cm_logit[1][1]+cm_logit[1][0])]
  
# making the bar chart on the data
plt.bar(xaxis, yaxis)
  
# calling the function to add value labels
plt.xlabel("Type")
plt.ylabel("Ammount")
  
# giving title to the plot
plt.title("Precision vs Recall")
  
# visualizing the plot
plt.show()

#CONFUSION MATRIX

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d').set_title('Title)')

print(classification_report(y_test,y_pred))

#CLASSIFICATION REPORT

print(classification_report(y_test, y_pred))

#ROC CURVE


logit_roc_auc = roc_auc_score(y_test, logitres.predict(X_test))
fpr, tpr, thresholds = roc_curve(y_test, logitres.predict_proba(X_test)[:,1])
plt.figure()
plt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()